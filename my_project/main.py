
import os
from file_utils import get_files_in_folder, read_text_file, read_csv_file, write_csv_file
from text_utils import count_words, count_unique_words, calculate_ttr, get_most_common_words, count_lines, average_word_length, calculate_lexical_density

def analyze_single_text(filepath, filename):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª.
    
    Args:
        filepath (str): –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        filename (str): –ò–º—è —Ñ–∞–π–ª–∞
    
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
    text = read_text_file(filepath)
    
    if text.startswith("–û—à–∏–±–∫–∞"):
        print(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {filename}: {text}")
        return None
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    result = {
        'filename': filename,
        'word_count': count_words(text),
        'unique_words': count_unique_words(text),
        'ttr': calculate_ttr(text),
        'line_count': count_lines(text),
        'avg_word_length': average_word_length(text),
    }
    
    # –õ–µ–∫—Å–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (–µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞)
    try:
        lex_metrics = calculate_lexical_density(text)
        result.update({
            'lexical_density': lex_metrics['lexical_density'],
            'noun_density': lex_metrics['noun_density'],
            'adj_density': lex_metrics['adj_density'],
            'verb_density': lex_metrics['verb_density'],
        })
    except NameError:
        # –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è calculate_lexical_density –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞
        print(f"  ‚ö†Ô∏è –§—É–Ω–∫—Ü–∏—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è {filename}")
    
    return result

def analyze_corpus(corpus_folder='corpus'):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –ø–∞–ø–∫–µ, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    
    Args:
        corpus_folder (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'corpus')
    
    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    print("=" * 60)
    print("üìä –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤")
    print("=" * 60)
    
    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
    files = get_files_in_folder(corpus_folder, '.txt')
    
    if not files:
        print("‚ùå –§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return []
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
    
    # 2. –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É results, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    if not os.path.exists('results'):
        os.makedirs('results')
        print("üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ 'results/'")
    
    # 3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    all_results = []
    
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤:")
    for i, filename in enumerate(files, 1):
        print(f"  {i}/{len(files)}: {filename}... ", end="")
        
        filepath = os.path.join(corpus_folder, filename)
        result = analyze_single_text(filepath, filename)
        
        if result:
            all_results.append(result)
            print("‚úÖ")
        else:
            print("‚ùå")
    
    # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    metadata = {}
    metadata_path = 'data/metadata.csv'
    
    if os.path.exists(metadata_path):
        print(f"\nüìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ {metadata_path}...")
        metadata_list = read_csv_file(metadata_path)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        for item in metadata_list:
            if 'filename' in item:
                metadata[item['filename']] = item
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(metadata)} –∑–∞–ø–∏—Å–µ–π")
    else:
        print(f"\n‚ö†Ô∏è –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {metadata_path}")
        print("  –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
    
    # 5. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    enriched_results = []
    for result in all_results:
        filename = result['filename']
        enriched_result = result.copy()
        
        if filename in metadata:
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            enriched_result.update({
                'title': metadata[filename].get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                'author': metadata[filename].get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                'year': metadata[filename].get('year', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                'genre': metadata[filename].get('genre', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
            })
        else:
            # –ï—Å–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∑–∞–ø–æ–ª–Ω—è–µ–º –∑–∞–≥–ª—É—à–∫–∞–º–∏
            enriched_result.update({
                'title': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
                'author': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
                'year': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
                'genre': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
            })
        
        enriched_results.append(enriched_result)
    
    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è CSV
    headers = [
        'filename', 'title', 'author', 'year', 'genre',
        'word_count', 'unique_words', 'ttr', 'line_count',
        'avg_word_length', 'lexical_density', 'noun_density',
        'adj_density', 'verb_density'
    ]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
    available_headers = []
    for header in headers:
        if any(header in result for result in enriched_results):
            available_headers.append(header)
    
    # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ (—Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è)
    csv_data = []
    for result in enriched_results:
        row = []
        for header in available_headers:
            row.append(result.get(header, ''))
        csv_data.append(row)

    write_csv_file("results/statistics.csv", csv_data, available_headers)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results/statistics.csv")
    
    # 7. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
    generate_report(enriched_results, corpus_folder)
    
    # 8. –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print_summary(enriched_results)
    
    return enriched_results

def generate_report(results, corpus_folder):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.
    
    Args:
        results (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        corpus_folder (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –∫–æ—Ä–ø—É—Å–∞
    """
    if not results:
        return
    
    report_lines = []
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á–µ—Ç–∞
    report_lines.append("=" * 60)
    report_lines.append("üìä –û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –¢–ï–ö–°–¢–û–í–û–ì–û –ö–û–†–ü–£–°–ê")
    report_lines.append("=" * 60)
    report_lines.append(f"–ü–∞–ø–∫–∞ –∫–æ—Ä–ø—É—Å–∞: {corpus_folder}")
    report_lines.append(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {os.path.basename(corpus_folder)}")
    report_lines.append("")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    report_lines.append("üìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    report_lines.append("-" * 40)
    
    total_files = len(results)
    total_words = sum(r.get('word_count', 0) for r in results)
    total_unique = sum(r.get('unique_words', 0) for r in results)
    avg_ttr = sum(r.get('ttr', 0) for r in results) / total_files if total_files > 0 else 0
    
    report_lines.append(f"  –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
    report_lines.append(f"  –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words:,}")
    report_lines.append(f"  –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {total_unique:,}")
    report_lines.append(f"  –°—Ä–µ–¥–Ω–∏–π TTR: {avg_ttr:.4f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if any('lexical_density' in r for r in results):
        avg_lex = sum(r.get('lexical_density', 0) for r in results) / total_files
        report_lines.append(f"  –°—Ä–µ–¥–Ω—è—è –ª–µ–∫—Å–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {avg_lex:.2%}")
        report_lines.append("")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–∞–π–ª–∞–º
    report_lines.append("")
    report_lines.append("üìã –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–ê–ú:")
    report_lines.append("=" * 60)
    
    for i, result in enumerate(results, 1):
        report_lines.append(f"\n{i}. üìÑ {result.get('filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        report_lines.append("-" * 40)
        
        if 'title' in result:
            report_lines.append(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {result.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        if 'author' in result:
            report_lines.append(f"   –ê–≤—Ç–æ—Ä: {result.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        if 'year' in result:
            report_lines.append(f"   –ì–æ–¥: {result.get('year', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        
        report_lines.append(f"   –°–ª–æ–≤: {result.get('word_count', 0):,}")
        report_lines.append(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {result.get('unique_words', 0):,}")
        report_lines.append(f"   TTR: {result.get('ttr', 0):.4f}")
        report_lines.append(f"   –°—Ç—Ä–æ–∫: {result.get('line_count', 0):,}")
        report_lines.append(f"   –°—Ä. –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {result.get('avg_word_length', 0):.2f}")
        
        if 'lexical_density' in result:
            report_lines.append(f"   –õ–µ–∫—Å. –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {result.get('lexical_density', 0):.2%}")
    
    # –í—ã–≤–æ–¥—ã
    report_lines.append("\n" + "=" * 60)
    report_lines.append("üí° –í–´–í–û–î–´ –ò –ù–ê–ë–õ–Æ–î–ï–ù–ò–Ø:")
    report_lines.append("=" * 60)
    
    if results:
        # –°–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª
        biggest_file = max(results, key=lambda x: x.get('word_count', 0))
        report_lines.append(f"‚Ä¢ –°–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {biggest_file.get('filename')} "
                          f"({biggest_file.get('word_count', 0):,} —Å–ª–æ–≤)")
        
        # –°–∞–º—ã–π –ª–µ–∫—Å–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π
        most_diverse = max(results, key=lambda x: x.get('ttr', 0))
        report_lines.append(f"‚Ä¢ –°–∞–º—ã–π –ª–µ–∫—Å–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π: {most_diverse.get('filename')} "
                          f"(TTR: {most_diverse.get('ttr', 0):.4f})")
        
        # –°–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è –ª–µ–∫—Å–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        if any('lexical_density' in r for r in results):
            most_dense = max(results, key=lambda x: x.get('lexical_density', 0))
            report_lines.append(f"‚Ä¢ –ù–∞–∏–±–æ–ª—å—à–∞—è –ª–µ–∫—Å–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {most_dense.get('filename')} "
                              f"({most_dense.get('lexical_density', 0):.2%})")
        
        # –ü–æ –∞–≤—Ç–æ—Ä–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
        authors = {}
        for result in results:
            author = result.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            if author not in authors:
                authors[author] = []
            authors[author].append(result)
        
        if len(authors) > 1:
            report_lines.append(f"\n‚Ä¢ –í—Å–µ–≥–æ –∞–≤—Ç–æ—Ä–æ–≤: {len(authors)}")
            for author, files in authors.items():
                report_lines.append(f"  - {author}: {len(files)} —Ñ–∞–π–ª–æ–≤")
    
    report_lines.append("\n" + "=" * 60)
    report_lines.append("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    report_lines.append("=" * 60)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
    report_content = "\n".join(report_lines)
    write_csv_file("results/report.txt", [{'report': report_content}], ['report'])
    
    # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    with open("results/report.txt", "w", encoding="utf-8") as f:
        f.write(report_content)
    
    print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ results/report.txt")

def print_summary(results):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –∫–æ–Ω—Å–æ–ª—å.
    
    Args:
        results (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    if not results:
        return
    
    print("\n" + "=" * 60)
    print("üìä –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 60)
    
    total_files = len(results)
    total_words = sum(r.get('word_count', 0) for r in results)
    total_unique = sum(r.get('unique_words', 0) for r in results)
    
    print(f"üìÅ –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
    print(f"üî§ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {total_words:,}")
    print(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {total_unique:,}")
    
    if total_files > 0:
        avg_words = total_words / total_files
        print(f"üìä –°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ —Å–ª–æ–≤ –≤ —Ñ–∞–π–ª–µ: {avg_words:,.2f}")
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
    if any('lexical_density' in r for r in results):
        total_lex = sum(r.get('lexical_density', 0) for r in results)
        avg_lex = total_lex / total_files if total_files > 0 else 0
        print(f"üìà –°—Ä–µ–¥–Ω—è—è –ª–µ–∫—Å–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {avg_lex:.2%}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã."""
    print("=" * 60)
    print("üìö –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞")
    print("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏ corpus
    corpus_folder = 'corpus'
    if not os.path.exists(corpus_folder):
        print(f"‚ùå –ü–∞–ø–∫–∞ '{corpus_folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞–ø–∫–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
        return

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞
    results = analyze_corpus(corpus_folder)

    if results:
        print("\n" + "=" * 60)
        print("üéâ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("=" * 60)
        print("üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/'")
        print("   - statistics.csv: –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ñ–∞–π–ª–∞–º")
        print("   - report.txt: –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å –≤—ã–≤–æ–¥–∞–º–∏")
    else:
        print("\n‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–ø—É—Å–µ.")

if __name__ == '__main__':
    main()

if __name__ == '__main__':
    main()
